{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Text on Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides some examples of how to load and visualise the text outputs of MapReader. \n",
    "\n",
    "We focus on the geogrpaphy of railspace and labels describing railspace on maps.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the patch predictions for railspace and railspace + buildings (codes 01 and 03)\n",
    "predictions = geopd.read_file(\"/Users/kasparbeelen/Documents/LwM/imago-mundi/mapreader_maptext_outputs/MapReader_Data_SIGSPATIAL_2022/outputs/label_01_03/pred_01_03_keep_01_0250.csv\")\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gepjson with spotted text\n",
    "# we use the file that has been converted to point data\n",
    "spotted_text = geopd.read_file(\"/Users/kasparbeelen/Documents/LwM/imago-mundi/mapreader_maptext_outputs/geo_predictions_deduplicated_point.json\")\n",
    "spotted_text.to_crs(epsg=27700, inplace=True)\n",
    "spotted_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotted_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotted_text['text_cleaned'] = spotted_text['text'].apply(lambda x: x.lower().strip().replace(\"(\", \"\").replace(\")\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering labels\n",
    "\n",
    "Here we discard the following labels\n",
    "- those starting and ending with #\n",
    "- those starting < or ending with >\n",
    "- numbers after stripping the dot\n",
    "\n",
    "The we lowercase all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter spotted to reduce noise labels\n",
    "filter_labels = lambda w : (w.endswith('#') or w.startswith('#') or w.endswith('>') or w.startswith('<') or w.strip('.').isdigit())\n",
    "spotted_text_filtered = spotted_text[~spotted_text.apply(lambda x: filter_labels(x['text_cleaned']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "spotted_text_filtered = spotted_text_filtered.drop_duplicates(subset=['patch_id','geometry','text']).reset_index(drop=True)\n",
    "\n",
    "spotted_text_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the dataframes\n",
    "print(f\"Railspace predictions shape: {predictions.shape}\")\n",
    "print(f\"Spotted text original shape: {spotted_text.shape} filtered {spotted_text_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotted_text_filtered.text_cleaned.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain patch predictions for maps that are in the spotted text data\n",
    "text_map_ids = list(spotted_text.image_id.unique())\n",
    "print('number of maps', len(text_map_ids))\n",
    "# filter to those maps for which we have spotted text\n",
    "predictions_red = predictions[predictions['parent_id'].isin(text_map_ids)]\n",
    "predictions_red.shape,predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add geometry to the patch predictions\n",
    "predictions_red['geometry'] = geopd.points_from_xy(predictions_red.center_lon, predictions_red.center_lat)\n",
    "predictions_red.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the projection to the same as the spotted text\n",
    "predictions_red.crs = \"epsg:4326\"\n",
    "predictions_red.to_crs(epsg=27700, inplace=True) # 27700\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotted_text[spotted_text.distance(predictions_red.iloc[1001].geometry) < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xyzservices as xyz\n",
    "tiles = xyz.TileProvider(\n",
    "    name=\"OS 2nd Edition - 6 inch\",\n",
    "    url=\"https://api.maptiler.com/tiles/uk-osgb1888/{z}/{x}/{y}?key=5f6FYax2HhTa0Z9RfXsp\",\n",
    "    attribution=\"NLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check by eyeballing some results\n",
    "# Important: you might have to install a few packages to get this to work\n",
    "# check the error message if it does not work\n",
    "spotted_text[spotted_text.distance(predictions_red.iloc[121].geometry) < 100].explore(tiles=tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect labels close to railspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell identifies the text that falls within the 100m of railspace patches. Text within this distance is stored as \"adjacent text\" and any other text is stored as \"other text\".\n",
    "\n",
    "You can **skip** this cell if you don't want to change the setting and go the next cell where we load the railspace labels which we saved in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "adjacent_text = [] # here we store labels close to the target category, i.e. railspace\n",
    "\n",
    "distance = 100 # maximum distance in meters between patch centroid and text centroid\n",
    "\n",
    "for i,row in tqdm(predictions_red.iterrows(), total=predictions_red.shape[0]):\n",
    "    # get text within a certain distance from the patch centroid\n",
    "    # get the set of text labels\n",
    "    labels = spotted_text_filtered[spotted_text_filtered.distance(row.geometry) <= distance].text_cleaned.tolist()\n",
    "    adjacent_text.extend(labels)\n",
    "\n",
    "# save the adjacent text to a txt file\n",
    "with open('adjacent_txt.txt', 'w') as out_txt:\n",
    "    out_txt.write('\\n'.join(adjacent_text))\n",
    "\n",
    "print('Railspace labels',len(adjacent_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotted_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important!\n",
    "Skip to this cell if you don't want to run the code for finding adjacent text. \n",
    "\n",
    "\n",
    "We just load the railspace labels as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adjacent_txt.txt', 'r') as in_txt:\n",
    "    adjacent_text = [i.strip() for i in in_txt.readlines()]\n",
    "adjacent_text[:10], len(adjacent_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute proportional difference \n",
    "\n",
    "We compute the proportional difference between the labels on railspace patch versus the 'background' i.e. all labels on the maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text labels on the maps\n",
    "all_text = spotted_text_filtered.text_cleaned.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and probabilities for the text labels for the railspace category\n",
    "railspace_text_freq =  Counter([i.lower() for i in adjacent_text])\n",
    "railspace_text_prob = {k: v/ sum(railspace_text_freq.values()) for k,v in railspace_text_freq.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and probabilities of the text labels for all the maps\n",
    "all_text_freq =  Counter([i.lower() for i in all_text])\n",
    "all_text_prob = {k: v/ sum(all_text_freq.values()) for k,v in all_text_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare both absoluate counts and probabilities of a give word\n",
    "word = 'railway'\n",
    "print(railspace_text_freq[word], all_text_freq[word])\n",
    "print(railspace_text_prob[word], all_text_prob[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the proportional difference between the probabilities of the text labels \n",
    "# in railspace and all the maps\n",
    "proportional_difference = sorted(\n",
    "                                {w: railspace_text_prob.get(w,0) - all_text_prob.get(w,0) \n",
    "                                        for w in all_text_prob.keys()}.items(), \n",
    "                                    key=lambda x: x[1], \n",
    "                                    reverse=True\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Railscape labels')\n",
    "print(proportional_difference[:5])\n",
    "print('Other labels')\n",
    "print(proportional_difference[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot proportional difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(proportional_difference[:20]).plot(kind='bar', x=0, y=1, legend=False, \n",
    "                            title='Top 20 terms in Building labels', \n",
    "                            xlabel='Term', ylabel='Difference in probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(proportional_difference[-20:]).plot(kind='bar', x=0, y=1, legend=False, \n",
    "                            title='Top 20 terms in Other labels', \n",
    "                            xlabel='Term', ylabel='Difference in probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what some of the abbreviations mean, please go to the NLS website: https://maps.nls.uk/os/abbrev/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visalizing the semantic of text on maps\n",
    "\n",
    "In the visualization below we encode each label to a vector using BERT-type language model. This generates a vector for each labels that approximates the 'meaning' of this label. Then we visualize these embeddigns in two dimensional space where you can explore the different semantic regions of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to run if you have not yet installed sentence-transformers, scikit-learn and plotly\n",
    "#!pip install -U -q sentence-transformers scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import plotly.express as px\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text labels above a certain threshold to obtain the railspace labels\n",
    "threshold = 0.0001 # i.e. labels with prob > threshold are in railspace\n",
    "railspace_labels = [w for w,v in proportional_difference if v > threshold]\n",
    "len(railspace_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the 'railway' labels on the map\n",
    "spotted_text[spotted_text.text_cleaned.isin(railspace_labels)].explore(tiles=tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all other labels based on the inverse of the railspace threshold\n",
    " # i.e. labels with prob < -threshold are in the other category\n",
    "other_labels = [w for w,v in proportional_difference if v < threshold*-1]\n",
    "len(other_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the railspace labels\n",
    "# load pre-trained sentence transformer model\n",
    "# you can plug in the BLERT model here but for TSNES we better use the distilbert model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens') # Livingwithmachines/bert_1760_1900\n",
    "\n",
    "# encode the sentences\n",
    "railspace_sentence_embeddings = model.encode(railspace_labels)\n",
    "\n",
    "# perform dimensionality reduction using TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(railspace_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the labels in 2D scatter plot using the probability difference as size\n",
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['text'] = railspace_labels\n",
    "data['size'] = [np.log(all_text_freq[w]) for w in railspace_labels]\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', size='size', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the railspace and other labels\n",
    "labels = railspace_labels + other_labels\n",
    "sentence_embeddings = model.encode(labels)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# visualize the labels in 2D scatter plot using the probability difference as size\n",
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['color'] = ['railspace']* len(railspace_labels) + ['other']* len(other_labels)\n",
    "data['text'] = labels\n",
    "data['size'] = [np.log(all_text_freq[w]) for w in labels]\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', size='size', color='color', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all labels higher than a certain threshold\n",
    "freq_threshold = 5\n",
    "labels = [w for w in list(set(all_text)) if all_text_freq[w]> freq_threshold]; print(len(labels))\n",
    "sentence_embeddings = model.encode(labels)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# visualize the labels in 2D scatter plot using the probability difference as size\n",
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['color'] = ['railspace' if w in railspace_labels  else 'other' for w in labels]\n",
    "data['text'] = labels\n",
    "data['size'] = [np.log(all_text_freq[w]) for w in labels]\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', size='size', color='color', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantice Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embedding model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens') # Livingwithmachines/bert_1760_1900 | distilbert-base-nli-mean-tokens\n",
    "# encode the railspace labels\n",
    "railspace_sentence_embeddings = model.encode(railspace_labels)\n",
    "# create a railspaces embedding, feel free to add other labels\n",
    "railspace = model.encode(['rail railway'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cosine distance between the railspaces labels and the labels\n",
    "data = pd.DataFrame(railspace_labels,columns=['text'])\n",
    "data['sem_dist_to_rail'] = cosine_distances(railspace_sentence_embeddings, railspace.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('sem_dist_to_rail').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('sem_dist_to_rail').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the other labels\n",
    "other_sentence_embeddings = model.encode(other_labels)\n",
    "# what are the closest labels to the railspace embedding\n",
    "pd.DataFrame.from_records([other_labels,\n",
    "              list(cosine_distances(other_sentence_embeddings, railspace.reshape(1, -1)).reshape(-1))],\n",
    "              ).T.sort_values(1).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26835/26835 [56:59<00:00,  7.85it/s]    \n"
     ]
    }
   ],
   "source": [
    "# important this cell takes a while to run\n",
    "# you can simply ignore and load the json file I've shared with you\n",
    "tqdm.pandas()\n",
    "adjacent_texts = [] # here we store labels close to the target category, i.e. railspace\n",
    "\n",
    "distance = 250 # maximum distance in meters between patch centroid and text centroid\n",
    "\n",
    "for i,row in tqdm(predictions_red.iterrows(), total=predictions_red.shape[0]):\n",
    "    # get text within a certain distance from the patch centroid\n",
    "    # get the set of text labels\n",
    "    labels = list(set(spotted_text_filtered[spotted_text_filtered.distance(row.geometry) <= distance].text_cleaned.tolist()))\n",
    "    labels = sorted(labels)\n",
    "    # add the labels as a list sorted alphabetically\n",
    "    adjacent_texts.append(labels)\n",
    "\n",
    "\n",
    "with open('adjacent_texts.json', 'w') as out_txt:\n",
    "    json.dump(adjacent_texts, out_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels from json\n",
    "with open('adjacent_texts.json', 'r') as in_txt:\n",
    "    adjacent_texts_json = json.load(in_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the list to remove short and non-alphabetic labels\n",
    "# this is not ideal as it remove abbreviations and other useful information\n",
    "# especially in the more denser urban areas, however we use it as a simple experiment\n",
    "# if you want to keep all labels, just comment out the following line\n",
    "labels_sentence = [' '.join([w for w in a if (len(w) > 2) and w.isalpha()]) for a in adjacent_texts]\n",
    "# and uncomment the following line\n",
    "# labels_sentence = [' '.join([w for w in a]) for a in adjacent_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embedding model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens') # Livingwithmachines/bert_1760_1900 | distilbert-base-nli-mean-tokens\n",
    "# encode the railspace \"sentences\", i.e. the list of alphabetically sorted labels\n",
    "railspace_sentence_embeddings = model.encode(labels_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26835, 768), (26835, 19))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "railspace_sentence_embeddings.shape, predictions_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit the kmeans model to the railspace embeddings\n",
    "# we use here 10 clusters, feel free to change the number of clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=0, n_init=\"auto\").fit(railspace_sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kasparbeelen/anaconda3/envs/mapreader/lib/python3.9/site-packages/geopandas/geodataframe.py:1528: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/Users/kasparbeelen/anaconda3/envs/mapreader/lib/python3.9/site-packages/geopandas/geodataframe.py:1528: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add labels to the predictions_red dataframe\n",
    "predictions_red['cluster'] = kmeans.labels_\n",
    "predictions_red['labels'] = labels_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the different clusters on the map\n",
    "predictions_red[[ 'geometry', 'cluster', 'laberels']].explore(column='cluster', tiles=tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot labels/sentences in semantic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 78.65548 , -73.02942 ],\n",
       "       [ 23.085203, -60.233402],\n",
       "       [ 78.75422 , -72.95126 ],\n",
       "       ...,\n",
       "       [ 60.639362,   7.259105],\n",
       "       [ 67.37914 ,   7.27327 ],\n",
       "       [ 67.30581 ,   7.260923]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(railspace_sentence_embeddings)\n",
    "embeddings_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['text'] = labels_sentence\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
