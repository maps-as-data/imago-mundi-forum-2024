{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Railspace Text + Patch Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides some examples of how to load and visualise the text outputs of MapReader. \n",
    "\n",
    "We focus on the geography of railspace and labels describing railspace on maps.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Load the patch predictions for railspace and railspace + buildings (codes 01 and 03)\n",
    "predictions = geopd.read_file(\"/Users/kmcdonough/Github/MapReader_all/imago_mundi_text_essay/imago_mundi_data/pred_01_03_keep_01_0250.csv\")\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 load the geojson with spotted text\n",
    "# we use the file that has been converted to point data\n",
    "spotted_text = geopd.read_file(\"/Users/kmcdonough/Github/MapReader_all/imago_mundi_text_essay/imago_mundi_data/geo_predictions_deduplicated_point.json\")\n",
    "spotted_text.to_crs(epsg=27700, inplace=True)\n",
    "spotted_text.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 view spotted_text df\n",
    "\n",
    "spotted_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 clean spotted_text \n",
    "\n",
    "spotted_text['text_cleaned'] = spotted_text['text'].apply(lambda x: x.lower().strip().replace(\"(\", \"\").replace(\")\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter labels\n",
    "\n",
    "Here we discard the following labels\n",
    "- those starting and ending with #\n",
    "- those starting < or ending with >\n",
    "- numbers after stripping the dot\n",
    "\n",
    "The we lowercase all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 filter spotted to reduce noise labels\n",
    "\n",
    "filter_labels = lambda w : (w.endswith('#') or w.startswith('#') or w.endswith('>') or w.startswith('<') or w.strip('.').isdigit())\n",
    "spotted_text_filtered = spotted_text[~spotted_text.apply(lambda x: filter_labels(x['text_cleaned']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 remove duplicates\n",
    "spotted_text_filtered = spotted_text_filtered.drop_duplicates(subset=['patch_id','geometry','text']).reset_index(drop=True)\n",
    "\n",
    "spotted_text_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 print the shape of the dataframes\n",
    "print(f\"Railspace predictions shape: {predictions.shape}\")\n",
    "print(f\"Spotted text original shape: {spotted_text.shape} filtered {spotted_text_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 count text_cleaned values\n",
    "spotted_text_filtered.text_cleaned.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Retain patch predictions for maps that are in the spotted text data\n",
    "text_map_ids = list(spotted_text.image_id.unique())\n",
    "print('number of maps', len(text_map_ids))\n",
    "# filter to those maps for which we have spotted text\n",
    "predictions_red = predictions[predictions['parent_id'].isin(text_map_ids)]\n",
    "predictions_red.shape,predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 add geometry to the patch predictions\n",
    "predictions_red['geometry'] = geopd.points_from_xy(predictions_red.center_lon, predictions_red.center_lat)\n",
    "predictions_red.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 View predictions_red\n",
    "predictions_red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 convert the projection to the same as the spotted text\n",
    "predictions_red.crs = \"epsg:4326\"\n",
    "predictions_red.to_crs(epsg=27700, inplace=True) # 27700\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5\n",
    "spotted_text[spotted_text.distance(predictions_red.iloc[1001].geometry) < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Import map tiles from NLS tileserver\n",
    "\n",
    "import xyzservices as xyz\n",
    "tiles = xyz.TileProvider(\n",
    "    name=\"OS 2nd Edition - 6 inch\",\n",
    "    url=\"https://api.maptiler.com/tiles/uk-osgb1888/{z}/{x}/{y}?key=5f6FYax2HhTa0Z9RfXsp\",\n",
    "    attribution=\"NLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELP - would like to have a plot of these patches so people can see data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 View spotted_text on map\n",
    "\n",
    "# double check by eyeballing some results\n",
    "# Important: you might have to install a few packages to get this to work\n",
    "# check the error message if it does not work\n",
    "spotted_text[spotted_text.distance(predictions_red.iloc[121].geometry) < 100].explore(tiles=tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collect labels close to railspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell identifies the text that falls within the 100m of railspace patches. Text within this distance is stored as \"adjacent text\" and any other text is stored as \"other text\".\n",
    "\n",
    "**SKIP** this cell if you don't want to change the setting and go the next Section 5 \"Load Adjacent Text DataFrame\" where we load the railspace labels which we saved in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 Calculate adjacent labels\n",
    "\n",
    "tqdm.pandas()\n",
    "adjacent_text = [] # here we store labels close to the target category, i.e. railspace\n",
    "\n",
    "distance = 100 # maximum distance in meters between patch centroid and text centroid\n",
    "\n",
    "for i,row in tqdm(predictions_red.iterrows(), total=predictions_red.shape[0]):\n",
    "    # get text within a certain distance from the patch centroid\n",
    "    # get the set of text labels\n",
    "    labels = spotted_text_filtered[spotted_text_filtered.distance(row.geometry) <= distance].text_cleaned.tolist()\n",
    "    adjacent_text.extend(labels)\n",
    "\n",
    "# save the adjacent text to a txt file\n",
    "with open('adjacent_txt.txt', 'w') as out_txt:\n",
    "    out_txt.write('\\n'.join(adjacent_text))\n",
    "\n",
    "print('Railspace labels',len(adjacent_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 View spotted text df\n",
    "\n",
    "spotted_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View adjacent_text\n",
    "\n",
    "adjacent_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Open Adjacent Text DataFrame\n",
    "Skip to this cell if you don't want to run the code for finding adjacent text. \n",
    "\n",
    "\n",
    "We just load the railspace labels as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 load adjacent text df\n",
    "\n",
    "with open('adjacent_txt.txt', 'r') as in_txt:\n",
    "    adjacent_text = [i.strip() for i in in_txt.readlines()]\n",
    "adjacent_text[:10], len(adjacent_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute proportional difference \n",
    "\n",
    "We compute the proportional difference between the labels on railspace patch versus the 'background' i.e. all labels on the maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 get the text labels on the maps\n",
    "all_text = spotted_text_filtered.text_cleaned.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 get counts and probabilities for the text labels for the railspace category\n",
    "railspace_text_freq =  Counter([i.lower() for i in adjacent_text])\n",
    "railspace_text_prob = {k: v/ sum(railspace_text_freq.values()) for k,v in railspace_text_freq.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 get counts and probabilities of the text labels for all the maps\n",
    "all_text_freq =  Counter([i.lower() for i in all_text])\n",
    "all_text_prob = {k: v/ sum(all_text_freq.values()) for k,v in all_text_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 compare both absolute counts and probabilities of a given word\n",
    "word = 'railway'\n",
    "print(railspace_text_freq[word], all_text_freq[word])\n",
    "print(railspace_text_prob[word], all_text_prob[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 compute the proportional difference between the probabilities of the text labels \n",
    "# in railspace and all the maps\n",
    "proportional_difference = sorted(\n",
    "                                {w: railspace_text_prob.get(w,0) - all_text_prob.get(w,0) \n",
    "                                        for w in all_text_prob.keys()}.items(), \n",
    "                                    key=lambda x: x[1], \n",
    "                                    reverse=True\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 print top labels\n",
    "\n",
    "print('Railscape labels')\n",
    "print(proportional_difference[:5])\n",
    "print('Other labels')\n",
    "print(proportional_difference[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot proportional difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 plot top railspace labels\n",
    "\n",
    "pd.DataFrame(proportional_difference[:20]).plot(kind='bar', x=0, y=1, legend=False, \n",
    "                            title='Top 20 terms in Railspace labels', \n",
    "                            xlabel='Term', ylabel='Difference in probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 plot top non-railspace labels\n",
    "\n",
    "pd.DataFrame(proportional_difference[-20:]).plot(kind='bar', x=0, y=1, legend=False, \n",
    "                            title='Top 20 terms in Other labels', \n",
    "                            xlabel='Term', ylabel='Difference in probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what some of the abbreviations mean, please go to the NLS website: https://maps.nls.uk/os/abbrev/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Visualizing the semantics of text on maps\n",
    "\n",
    "In the visualization below we encode each label to a vector using BERT-type language model. This generates a vector for each labels that approximates the 'meaning' of this label. Then we visualize these embeddigns in two dimensional space where you can explore the different semantic regions of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to run if you have not yet installed sentence-transformers, scikit-learn and plotly\n",
    "!pip install -U -q sentence-transformers scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import plotly.express as px\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 get all text labels above a certain threshold to obtain the railspace labels\n",
    "threshold = 0.0001 # i.e. labels with prob > threshold are in railspace\n",
    "railspace_labels = [w for w,v in proportional_difference if v > threshold]\n",
    "len(railspace_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 plot all the 'railway' labels on the map\n",
    "spotted_text[spotted_text.text_cleaned.isin(railspace_labels)].explore(tiles=tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 get all other labels based on the inverse of the railspace threshold\n",
    " # i.e. labels with prob < -threshold are in the other category\n",
    "other_labels = [w for w,v in proportional_difference if v < threshold*-1]\n",
    "len(other_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 embed the railspace labels\n",
    "# load pre-trained sentence transformer model\n",
    "# you can plug in the BLERT model here but for TSNES we better use the distilbert model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens') # Livingwithmachines/bert_1760_1900\n",
    "\n",
    "# encode the sentences\n",
    "railspace_sentence_embeddings = model.encode(railspace_labels)\n",
    "\n",
    "# perform dimensionality reduction using TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(railspace_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 visualize the labels in 2D scatter plot using the probability difference as size\n",
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['text'] = railspace_labels\n",
    "data['size'] = [np.log(all_text_freq[w]) for w in railspace_labels]\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', size='size', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 visualize the railspace and other labels\n",
    "labels = railspace_labels + other_labels\n",
    "sentence_embeddings = model.encode(labels)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# visualize the labels in 2D scatter plot using the probability difference as size\n",
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['color'] = ['railspace']* len(railspace_labels) + ['other']* len(other_labels)\n",
    "data['text'] = labels\n",
    "data['size'] = [np.log(all_text_freq[w]) for w in labels]\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', size='size', color='color', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.7 plot all labels higher than a certain threshold\n",
    "freq_threshold = 5\n",
    "labels = [w for w in list(set(all_text)) if all_text_freq[w]> freq_threshold]; print(len(labels))\n",
    "sentence_embeddings = model.encode(labels)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# visualize the labels in 2D scatter plot using the probability difference as size\n",
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['color'] = ['railspace' if w in railspace_labels  else 'other' for w in labels]\n",
    "data['text'] = labels\n",
    "data['size'] = [np.log(all_text_freq[w]) for w in labels]\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', size='size', color='color', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Semantic Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 create a railspace sentence embedding\n",
    "\n",
    "# load the embedding model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens') # Livingwithmachines/bert_1760_1900 | distilbert-base-nli-mean-tokens\n",
    "# encode the railspace labels\n",
    "railspace_sentence_embeddings = model.encode(railspace_labels)\n",
    "# create a railspaces embedding, feel free to add other labels\n",
    "railspace = model.encode(['rail railway'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 compute the cosine distance between the railspaces labels and the labels\n",
    "data = pd.DataFrame(railspace_labels,columns=['text'])\n",
    "data['sem_dist_to_rail'] = cosine_distances(railspace_sentence_embeddings, railspace.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 View data and semantic distance (head)\n",
    "\n",
    "data.sort_values('sem_dist_to_rail').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 View data and semantic distance (tail)\n",
    "\n",
    "data.sort_values('sem_dist_to_rail').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 encode the other labels\n",
    "other_sentence_embeddings = model.encode(other_labels)\n",
    "# what are the closest labels to the railspace embedding\n",
    "pd.DataFrame.from_records([other_labels,\n",
    "              list(cosine_distances(other_sentence_embeddings, railspace.reshape(1, -1)).reshape(-1))],\n",
    "              ).T.sort_values(1).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Clustering experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Create 'sentences' of railspace labels in patches\n",
    "\n",
    "# important this cell takes a while to run\n",
    "# you can simply ignore and load the json file I've shared with you\n",
    "tqdm.pandas()\n",
    "adjacent_texts = [] # here we store labels close to the target category, i.e. railspace\n",
    "\n",
    "distance = 250 # maximum distance in meters between patch centroid and text centroid\n",
    "\n",
    "for i,row in tqdm(predictions_red.iterrows(), total=predictions_red.shape[0]):\n",
    "    # get text within a certain distance from the patch centroid\n",
    "    # get the set of text labels\n",
    "    labels = list(set(spotted_text_filtered[spotted_text_filtered.distance(row.geometry) <= distance].text_cleaned.tolist()))\n",
    "    labels = sorted(labels)\n",
    "    # add the labels as a list sorted alphabetically\n",
    "    adjacent_texts.append(labels)\n",
    "\n",
    "\n",
    "with open('adjacent_texts.json', 'w') as out_txt:\n",
    "    json.dump(adjacent_texts, out_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels from json\n",
    "with open('adjacent_texts.json', 'r') as in_txt:\n",
    "    adjacent_texts_json = json.load(in_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 filter the list to remove short and non-alphabetic labels\n",
    "\n",
    "# this is not ideal as it remove abbreviations and other useful information\n",
    "# especially in the more denser urban areas, however we use it as a simple experiment\n",
    "# if you want to keep all labels, just comment out the following line\n",
    "labels_sentence = [' '.join([w for w in a if (len(w) > 2) and w.isalpha()]) for a in adjacent_texts]\n",
    "# and uncomment the following line\n",
    "# labels_sentence = [' '.join([w for w in a]) for a in adjacent_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 load the embedding model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens') # Livingwithmachines/bert_1760_1900 | distilbert-base-nli-mean-tokens\n",
    "# encode the railspace \"sentences\", i.e. the list of alphabetically sorted labels\n",
    "\n",
    "# Note to self: avoid 'sentence'. say set of (alphabetically sorted) spotted text within 250m of railspace patch centroid. \n",
    "# now, for each set of patchText, we examine how similar they are to each other and organise them into a (pre-determined) number of 'clusters' \n",
    "# e.g. a cluster is a group of sets of patchText that are most like each other\n",
    "\n",
    "railspace_sentence_embeddings = model.encode(labels_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "railspace_sentence_embeddings.shape, predictions_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10.4 fit the kmeans model to the railspace embeddings\n",
    "# we use here 4 clusters, feel free to change the number of clusters\n",
    "\n",
    "\n",
    "# we choose a method of clustering as one approach among many to organise the data, to see patterns in the data\n",
    "# this is connecting what we know about text and what we know about visual features on maps for the first time\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, random_state=0, n_init=\"auto\").fit(railspace_sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels to the predictions_red dataframe\n",
    "predictions_red['cluster'] = kmeans.labels_\n",
    "predictions_red['labels'] = labels_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 plot the different clusters on the map\n",
    "predictions_red[[ 'geometry', 'cluster', 'labels']].explore(column='cluster', tiles=tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.6 Understanding Clusters\n",
    "\n",
    "The cells below contain different ways of looking at the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels for each cluster in predictions_red\n",
    "cluster_labels = predictions_red.groupby('cluster')['labels'].apply(list).reset_index()\n",
    "print(cluster_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count labels per cluster\n",
    "cluster_labels['count'] = cluster_labels['labels'].apply(lambda x: len(x)) \n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique labels per cluster\n",
    "cluster_labels['unique_count'] = cluster_labels['labels'].apply(lambda x: len(set(x)))\n",
    "print(cluster_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame with list of labels and their counts per cluster\n",
    "cluster_labels_long = cluster_labels.explode('labels')\n",
    "print(cluster_labels_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels column into individual words\n",
    "\n",
    "cluster_labels_long['words'] = cluster_labels_long['labels'].str.split()\n",
    "print(cluster_labels_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, count most frequent words and sort by frequency\n",
    "cluster_words = cluster_labels_long.groupby('cluster')['words'].sum().apply(Counter).reset_index()\n",
    "print(cluster_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the most words per cluster in a bar chart\n",
    "for i, row in cluster_words.iterrows():\n",
    "    data = pd.DataFrame(row['words'].most_common(15), columns=['word', 'count'])\n",
    "    data.plot(kind='bar', x='word', y='count', title=f'Cluster {row[\"cluster\"]}', legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clusters\n",
    "predictions_red.plot(column='cluster', legend=True, markersize=10, cmap='tab20', figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_per_cluster = predictions_red['cluster'].value_counts()\n",
    "print(patches_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2 = predictions_red['cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot labels/sentences in semantic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(railspace_sentence_embeddings)\n",
    "embeddings_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['text'] = labels_sentence\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapreader310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
